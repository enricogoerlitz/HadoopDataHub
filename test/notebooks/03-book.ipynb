{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0X Notebook-Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import pandas as pd\n",
    "import connection as connsettings\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from etl.etl import HadoopStdETL\n",
    "from etl.clients import HiveClient, HDFileSystemClient\n",
    "from etl.datamodels import HostDataClass, TableDataClass\n",
    "from etl.enums import eHdfsFileType\n",
    "from etl.connectors import CsvConnector\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CsvConnector(\n",
       "\t'path=./database/datev.dbo.client.csv',\n",
       "\t'sep=|'\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = CsvConnector(path=\"./database/datev.dbo.client.csv\", sep=\"|\")\n",
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>Omicron Systems GmbH</td>\n",
       "      <td>Musterstraße 57, 10163 Berlin</td>\n",
       "      <td>cf uc: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Epic Solutions SE</td>\n",
       "      <td>Musterstraße 19, 10125 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>Quantum Consulting SE</td>\n",
       "      <td>Musterstraße 66, 10172 Berlin</td>\n",
       "      <td>cf uc: 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   name                        address change_field\n",
       "0  57   Omicron Systems GmbH  Musterstraße 57, 10163 Berlin     cf uc: 1\n",
       "1  19      Epic Solutions SE  Musterstraße 19, 10125 Berlin           cf\n",
       "2  66  Quantum Consulting SE  Musterstraße 66, 10172 Berlin     cf uc: 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Harmony Services GmbH</td>\n",
       "      <td>Musterstraße 30, 10136 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Echo Solutions SE</td>\n",
       "      <td>Musterstraße 17, 10123 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>Mu Enterprises AG</td>\n",
       "      <td>Musterstraße 50, 10156 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   name                        address change_field\n",
       "3  30  Harmony Services GmbH  Musterstraße 30, 10136 Berlin           cf\n",
       "4  17      Echo Solutions SE  Musterstraße 17, 10123 Berlin           cf\n",
       "5  50      Mu Enterprises AG  Musterstraße 50, 10156 Berlin           cf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Momentum Innovations SE</td>\n",
       "      <td>Musterstraße 49, 10155 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Apex Corporation GmbH</td>\n",
       "      <td>Musterstraße 3, 10109 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>88</td>\n",
       "      <td>Vortex Technologies SE</td>\n",
       "      <td>Musterstraße 88, 10194 Berlin</td>\n",
       "      <td>cf uc: 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     name                        address change_field\n",
       "6  49  Momentum Innovations SE  Musterstraße 49, 10155 Berlin           cf\n",
       "7   3    Apex Corporation GmbH   Musterstraße 3, 10109 Berlin           cf\n",
       "8  88   Vortex Technologies SE  Musterstraße 88, 10194 Berlin     cf uc: 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>Oasis Corporation Ltd.</td>\n",
       "      <td>Musterstraße 55, 10161 Berlin</td>\n",
       "      <td>cf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    name                        address change_field\n",
       "9  55  Oasis Corporation Ltd.  Musterstraße 55, 10161 Berlin           cf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch in conn.iterbatches(batchsize=3):\n",
    "    display(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_host = HostDataClass(host=connsettings.HIVE_HOST, port=connsettings.HIVE_PORT)\n",
    "hdfs_host = HostDataClass(host=connsettings.HDFS_HOST, port=connsettings.HDFS_PORT)\n",
    "hdfs_client = HDFileSystemClient(hdfs_host=hdfs_host, hdfs_username=\"enricogoerlitz\")\n",
    "hive_client = HiveClient(host=hive_host, thrift_port=connsettings.HIVE_THRIFT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START LOAD TO TMP\n",
      "UPDATE /edw/hive/psa/datev/dbo/client/client\n",
      "UPDATED DF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>COLDIFF_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>COLDIFF_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pk, guid, id, name, address, change_field, row_is_current, row_valid_to, row_is_deleted, row_filepath, row_filepath_tmp, is_batch_file, COLDIFF_KEY, guid_tmp, COLDIFF_KEY_TMP]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETED DF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>COLDIFF_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>COLDIFF_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88</td>\n",
       "      <td>8586908f-0ff8-4fec-957c-8102d0972b07</td>\n",
       "      <td>88</td>\n",
       "      <td>Vortex Technologies SE</td>\n",
       "      <td>Musterstraße 88, 10194 Berlin</td>\n",
       "      <td>cf uc: 2</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Vortex Technologies SE_cf uc: 2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pk                                  guid  id                    name  \\\n",
       "0  88  8586908f-0ff8-4fec-957c-8102d0972b07  88  Vortex Technologies SE   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 88, 10194 Berlin     cf uc: 2               1   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2100-12-31 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \\\n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1   \n",
       "\n",
       "                       COLDIFF_KEY guid_tmp COLDIFF_KEY_TMP  \n",
       "0  Vortex Technologies SE_cf uc: 2     None            None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMED DF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>581fef1a-b1e5-4f99-b7c3-17c4474fa54d</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>Quantum Consulting SE</td>\n",
       "      <td>Musterstraße 66, 10172 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8a83f545-c371-4579-8e02-4ca7df52da62</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>Mu Enterprises AG</td>\n",
       "      <td>Musterstraße 50, 10156 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6d16f3d8-2ef1-460d-a36d-507fa91b0d4c</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>Oasis Corporation Ltd.</td>\n",
       "      <td>Musterstraße 55, 10161 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>818b728c-ffee-43b6-b295-503b181cdc5d</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Echo Solutions SE</td>\n",
       "      <td>Musterstraße 17, 10123 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d56a8fc6-bd1e-4304-954c-aa9dab8e016c</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Epic Solutions SE</td>\n",
       "      <td>Musterstraße 19, 10125 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2d30b0a6-2a61-471a-9323-87c3e68393d3</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>Vortex Technologies SE</td>\n",
       "      <td>Musterstraße 88, 10194 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b0d6b07f-28af-4129-bd48-6386ff002557</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>Vortex Technologies SE</td>\n",
       "      <td>Musterstraße 88, 10194 Berlin</td>\n",
       "      <td>cf uc: 1</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8586908f-0ff8-4fec-957c-8102d0972b07</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>Vortex Technologies SE</td>\n",
       "      <td>Musterstraße 88, 10194 Berlin</td>\n",
       "      <td>cf uc: 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                    name  \\\n",
       "0  581fef1a-b1e5-4f99-b7c3-17c4474fa54d  66  66   Quantum Consulting SE   \n",
       "1  8a83f545-c371-4579-8e02-4ca7df52da62  50  50       Mu Enterprises AG   \n",
       "2  6d16f3d8-2ef1-460d-a36d-507fa91b0d4c  55  55  Oasis Corporation Ltd.   \n",
       "3  818b728c-ffee-43b6-b295-503b181cdc5d  17  17       Echo Solutions SE   \n",
       "4  d56a8fc6-bd1e-4304-954c-aa9dab8e016c  19  19       Epic Solutions SE   \n",
       "5  2d30b0a6-2a61-471a-9323-87c3e68393d3  88  88  Vortex Technologies SE   \n",
       "6  b0d6b07f-28af-4129-bd48-6386ff002557  88  88  Vortex Technologies SE   \n",
       "0  8586908f-0ff8-4fec-957c-8102d0972b07  88  88  Vortex Technologies SE   \n",
       "\n",
       "                         address change_field row_is_current  \\\n",
       "0  Musterstraße 66, 10172 Berlin           cf              0   \n",
       "1  Musterstraße 50, 10156 Berlin           cf              0   \n",
       "2  Musterstraße 55, 10161 Berlin           cf              0   \n",
       "3  Musterstraße 17, 10123 Berlin           cf              0   \n",
       "4  Musterstraße 19, 10125 Berlin           cf              0   \n",
       "5  Musterstraße 88, 10194 Berlin           cf              0   \n",
       "6  Musterstraße 88, 10194 Berlin     cf uc: 1              0   \n",
       "0  Musterstraße 88, 10194 Berlin     cf uc: 2              0   \n",
       "\n",
       "          row_valid_to row_is_deleted  \\\n",
       "0  2024-01-04 00:00:00              0   \n",
       "1  2024-01-04 00:00:00              0   \n",
       "2  2024-01-04 00:00:00              0   \n",
       "3  2024-01-04 00:00:00              1   \n",
       "4  2024-01-04 00:00:00              0   \n",
       "5  2024-01-04 00:00:00              0   \n",
       "6  2024-01-04 00:00:00              0   \n",
       "0  2024-01-04 00:00:00              1   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "3  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "4  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "5  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "6  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "0  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "\n",
       "                                    row_filepath_tmp is_batch_file  \n",
       "0  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "1  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "2  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "3  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "4  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "5  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "6  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  \n",
       "0  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVE PSA TO BCK\n",
      "MOVE TMP TO PSA\n",
      "DELETE BCK\n"
     ]
    }
   ],
   "source": [
    "conn = CsvConnector(path=\"./database/datev.dbo.client.csv\", sep=\"|\")\n",
    "\n",
    "etl = HadoopStdETL(\n",
    "    conn=conn,\n",
    "    hdfs_client=hdfs_client,\n",
    "    hive_client=hive_client,\n",
    "    tmp_path=\"/edw/hive/tmp/\",\n",
    "    bck_path=\"/edw/hive/bck_psa/\",\n",
    "    dist_path=\"/edw/hive/psa/\",\n",
    "    table=TableDataClass(\n",
    "        database=\"datev\",\n",
    "        schema=\"dbo\",\n",
    "        table_name=\"client\",\n",
    "        pk=[\"id\"]\n",
    "    ),\n",
    "    use_spark=True,\n",
    "    change_columns=[\"name\", \"change_field\"],\n",
    "    historize=True\n",
    ")\n",
    "\n",
    "etl.start(batchsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVE PSA TO BCK\n",
      "MOVE TMP TO PSA\n",
      "DELETE BCK\n"
     ]
    }
   ],
   "source": [
    "conn = CsvConnector(path=\"./database/datev.dbo.employee_pay.csv\", sep=\"|\")\n",
    "\n",
    "etl = HadoopStdETL(\n",
    "    conn=conn,\n",
    "    hdfs_client=hdfs_client,\n",
    "    hive_client=hive_client,\n",
    "    tmp_path=\"/edw/hive/tmp/\",\n",
    "    bck_path=\"/edw/hive/bck_psa/\",\n",
    "    dist_path=\"/edw/hive/psa/\",\n",
    "    table=TableDataClass(\n",
    "        database=\"datev\",\n",
    "        schema=\"dbo\",\n",
    "        table_name=\"employee_pay\",\n",
    "        pk=[\"transaction_date\", \"employee_id\",\n",
    "            \"client_id\", \"costcenter_id\", \"paytype_id\",\n",
    "            \"amount\"]\n",
    "    ),\n",
    "    use_spark=True,\n",
    "    change_columns=None,\n",
    "    historize=False\n",
    ")\n",
    "\n",
    "etl.start(batchsize=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START LOAD TO TMP\n",
      "UPDATE /edw/hive/psa/datev/dbo/employee_pay_hist/employee_pay_hist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/01/04 19:01:02 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.4 instead (on interface en0)\n",
      "24/01/04 19:01:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/04 19:01:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/04 19:01:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 3:==============>                                            (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATED DF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>client_id</th>\n",
       "      <th>costcenter_id</th>\n",
       "      <th>paytype_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>bdcolumn1</th>\n",
       "      <th>bdcolumn2</th>\n",
       "      <th>...</th>\n",
       "      <th>bdcolumn150</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>COLDIFF_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>COLDIFF_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pk, guid, transaction_date, employee_id, client_id, costcenter_id, paytype_id, amount, bdcolumn1, bdcolumn2, bdcolumn3, bdcolumn4, bdcolumn5, bdcolumn6, bdcolumn7, bdcolumn8, bdcolumn9, bdcolumn10, bdcolumn11, bdcolumn12, bdcolumn13, bdcolumn14, bdcolumn15, bdcolumn16, bdcolumn17, bdcolumn18, bdcolumn19, bdcolumn20, bdcolumn21, bdcolumn22, bdcolumn23, bdcolumn24, bdcolumn25, bdcolumn26, bdcolumn27, bdcolumn28, bdcolumn29, bdcolumn30, bdcolumn31, bdcolumn32, bdcolumn33, bdcolumn34, bdcolumn35, bdcolumn36, bdcolumn37, bdcolumn38, bdcolumn39, bdcolumn40, bdcolumn41, bdcolumn42, bdcolumn43, bdcolumn44, bdcolumn45, bdcolumn46, bdcolumn47, bdcolumn48, bdcolumn49, bdcolumn50, bdcolumn51, bdcolumn52, bdcolumn53, bdcolumn54, bdcolumn55, bdcolumn56, bdcolumn57, bdcolumn58, bdcolumn59, bdcolumn60, bdcolumn61, bdcolumn62, bdcolumn63, bdcolumn64, bdcolumn65, bdcolumn66, bdcolumn67, bdcolumn68, bdcolumn69, bdcolumn70, bdcolumn71, bdcolumn72, bdcolumn73, bdcolumn74, bdcolumn75, bdcolumn76, bdcolumn77, bdcolumn78, bdcolumn79, bdcolumn80, bdcolumn81, bdcolumn82, bdcolumn83, bdcolumn84, bdcolumn85, bdcolumn86, bdcolumn87, bdcolumn88, bdcolumn89, bdcolumn90, bdcolumn91, bdcolumn92, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 167 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETED DF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>client_id</th>\n",
       "      <th>costcenter_id</th>\n",
       "      <th>paytype_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>bdcolumn1</th>\n",
       "      <th>bdcolumn2</th>\n",
       "      <th>...</th>\n",
       "      <th>bdcolumn150</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>COLDIFF_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>COLDIFF_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pk, guid, transaction_date, employee_id, client_id, costcenter_id, paytype_id, amount, bdcolumn1, bdcolumn2, bdcolumn3, bdcolumn4, bdcolumn5, bdcolumn6, bdcolumn7, bdcolumn8, bdcolumn9, bdcolumn10, bdcolumn11, bdcolumn12, bdcolumn13, bdcolumn14, bdcolumn15, bdcolumn16, bdcolumn17, bdcolumn18, bdcolumn19, bdcolumn20, bdcolumn21, bdcolumn22, bdcolumn23, bdcolumn24, bdcolumn25, bdcolumn26, bdcolumn27, bdcolumn28, bdcolumn29, bdcolumn30, bdcolumn31, bdcolumn32, bdcolumn33, bdcolumn34, bdcolumn35, bdcolumn36, bdcolumn37, bdcolumn38, bdcolumn39, bdcolumn40, bdcolumn41, bdcolumn42, bdcolumn43, bdcolumn44, bdcolumn45, bdcolumn46, bdcolumn47, bdcolumn48, bdcolumn49, bdcolumn50, bdcolumn51, bdcolumn52, bdcolumn53, bdcolumn54, bdcolumn55, bdcolumn56, bdcolumn57, bdcolumn58, bdcolumn59, bdcolumn60, bdcolumn61, bdcolumn62, bdcolumn63, bdcolumn64, bdcolumn65, bdcolumn66, bdcolumn67, bdcolumn68, bdcolumn69, bdcolumn70, bdcolumn71, bdcolumn72, bdcolumn73, bdcolumn74, bdcolumn75, bdcolumn76, bdcolumn77, bdcolumn78, bdcolumn79, bdcolumn80, bdcolumn81, bdcolumn82, bdcolumn83, bdcolumn84, bdcolumn85, bdcolumn86, bdcolumn87, bdcolumn88, bdcolumn89, bdcolumn90, bdcolumn91, bdcolumn92, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 167 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMED DF:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVE PSA TO BCK\n",
      "MOVE TMP TO PSA\n",
      "DELETE BCK\n"
     ]
    }
   ],
   "source": [
    "conn = CsvConnector(path=\"./database/datev.dbo.employee_pay.csv\", sep=\"|\")\n",
    "\n",
    "dummy_cols = [\"transaction_date\", \"employee_id\",\n",
    "            \"client_id\", \"costcenter_id\", \"paytype_id\",\n",
    "            \"amount\"]\n",
    "etl = HadoopStdETL(\n",
    "    conn=conn,\n",
    "    hdfs_client=hdfs_client,\n",
    "    hive_client=hive_client,\n",
    "    tmp_path=\"/edw/hive/tmp/\",\n",
    "    bck_path=\"/edw/hive/bck_psa/\",\n",
    "    dist_path=\"/edw/hive/psa/\",\n",
    "    table=TableDataClass(\n",
    "        database=\"datev\",\n",
    "        schema=\"dbo\",\n",
    "        table_name=\"employee_pay_hist\",\n",
    "        pk=dummy_cols\n",
    "    ),\n",
    "    use_spark=True,\n",
    "    change_columns=dummy_cols,\n",
    "    historize=True\n",
    ")\n",
    "\n",
    "etl.start(batchsize=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>COLDIFF_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>COLDIFF_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>828cf089-6c36-4ae3-baaf-9e6a53fee5c3</td>\n",
       "      <td>91</td>\n",
       "      <td>Wavelength Ventures AG</td>\n",
       "      <td>Musterstraße 91, 10197 Berlin</td>\n",
       "      <td>cf uc: 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wavelength Ventures AG_cf uc: 2</td>\n",
       "      <td>b343c776-db21-4f15-90b8-da11f979f204</td>\n",
       "      <td>Wavelength Ventures AG_cf uc: 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>d4b7e05c-aa5a-43c1-ae2d-d4bf1237d7af</td>\n",
       "      <td>49</td>\n",
       "      <td>Momentum Innovations SE</td>\n",
       "      <td>Musterstraße 49, 10155 Berlin</td>\n",
       "      <td>cf uc: 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/UPDATE_0...</td>\n",
       "      <td>0</td>\n",
       "      <td>Momentum Innovations SE_cf uc: 2</td>\n",
       "      <td>9ff192b3-ec11-4017-8ff4-6875ab4d62f6</td>\n",
       "      <td>Momentum Innovations SE_cf uc: 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pk                                  guid  id                     name  \\\n",
       "0  91  828cf089-6c36-4ae3-baaf-9e6a53fee5c3  91   Wavelength Ventures AG   \n",
       "1  49  d4b7e05c-aa5a-43c1-ae2d-d4bf1237d7af  49  Momentum Innovations SE   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 91, 10197 Berlin     cf uc: 2               0   \n",
       "1  Musterstraße 49, 10155 Berlin     cf uc: 2               0   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2024-01-04 00:00:00               0   \n",
       "1  2024-01-04 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...   \n",
       "\n",
       "                                    row_filepath_tmp  is_batch_file  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...              0   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/UPDATE_0...              0   \n",
       "\n",
       "                        COLDIFF_KEY                              guid_tmp  \\\n",
       "0   Wavelength Ventures AG_cf uc: 2  b343c776-db21-4f15-90b8-da11f979f204   \n",
       "1  Momentum Innovations SE_cf uc: 2  9ff192b3-ec11-4017-8ff4-6875ab4d62f6   \n",
       "\n",
       "                    COLDIFF_KEY_TMP  \n",
       "0   Wavelength Ventures AG_cf uc: 3  \n",
       "1  Momentum Innovations SE_cf uc: 3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.read_parquet(\"/edw/hive/psa/datev/dbo/client/client/UPDATE_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_host = HostDataClass(host=connsettings.HIVE_HOST, port=connsettings.HIVE_PORT)\n",
    "hdfs_host = HostDataClass(host=connsettings.HDFS_HOST, port=connsettings.HDFS_PORT)\n",
    "hdfs_client = HDFileSystemClient(hdfs_host=hdfs_host, hdfs_username=\"enricogoerlitz\")\n",
    "hive_client = HiveClient(host=hive_host, thrift_port=connsettings.HIVE_THRIFT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TableDataClass(table_name='tmp_client', database='datev', schema=None, pk=None),\n",
       " TableDataClass(table_name='client', database='datev', schema=None, pk=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_table = TableDataClass(\"tmp_client\", \"datev\")\n",
    "table = TableDataClass(\"client\", \"datev\")\n",
    "\n",
    "tmp_table, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/01/04 18:07:32 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.4 instead (on interface en0)\n",
      "24/01/04 18:07:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/04 18:07:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `datev`.`tmp_client` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [datev, tmp_client], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m hive_client\u001b[38;5;241m.\u001b[39mcreate_spark_session()\n\u001b[0;32m----> 2\u001b[0m df_tmp_client \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM datev.tmp_client\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_client \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM datev.client WHERE is_batch_file = 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m base_columns \u001b[38;5;241m=\u001b[39m df_client\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `datev`.`tmp_client` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [datev, tmp_client], [], false\n"
     ]
    }
   ],
   "source": [
    "spark = hive_client.create_spark_session()\n",
    "df_tmp_client = spark.sql(f\"SELECT * FROM datev.tmp_client\")\n",
    "df_client = spark.sql(f\"SELECT * FROM datev.client WHERE is_batch_file = 1\")\n",
    "\n",
    "base_columns = df_client.columns\n",
    "base_columns_tmp = df_tmp_client.columns\n",
    "\n",
    "df_tmp_client.show()\n",
    "df_client.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+--------------------+------------+--------------+-------------------+--------------+--------------------+--------------------+-------------+--------------------+---+--------------------+--------------------+------------+--------------+-------------------+--------------+--------------------+--------------------+-------------+\n",
      "| pk|                guid| id|                name|             address|change_field|row_is_current|       row_valid_to|row_is_deleted|        row_filepath|    row_filepath_tmp|is_batch_file|                guid| id|                name|             address|change_field|row_is_current|       row_valid_to|row_is_deleted|        row_filepath|    row_filepath_tmp|is_batch_file|\n",
      "+---+--------------------+---+--------------------+--------------------+------------+--------------+-------------------+--------------+--------------------+--------------------+-------------+--------------------+---+--------------------+--------------------+------------+--------------+-------------------+--------------+--------------------+--------------------+-------------+\n",
      "| 49|56fb3c9f-e7f4-471...| 49|Momentum Innovati...|Musterstraße 49, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|645ff95f-7682-4ff...| 49|Momentum Innovati...|Musterstraße 49, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 92|f0c3f736-0924-407...| 92|    Xcel Ventures AG|Musterstraße 92, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|70466930-83c5-482...| 92|    Xcel Ventures AG|Musterstraße 92, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 91|abca0a8f-f907-473...| 91|Wavelength Ventur...|Musterstraße 91, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|15464f6f-402d-492...| 91|Wavelength Ventur...|Musterstraße 91, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 57|98b032ce-5ab8-4d8...| 57|Omicron Systems GmbH|Musterstraße 57, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|805656fe-4ef7-48e...| 57|Omicron Systems GmbH|Musterstraße 57, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 19|e6bbb303-dcf2-409...| 19|   Epic Solutions SE|Musterstraße 19, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|e1111c23-3f3e-432...| 19|   Epic Solutions SE|Musterstraße 19, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 66|d73b1652-55e3-40a...| 66|Quantum Consultin...|Musterstraße 66, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|e82722a2-35c0-431...| 66|Quantum Consultin...|Musterstraße 66, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 30|875138b2-d05b-4d6...| 30|Harmony Services ...|Musterstraße 30, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|cbec21af-3bb7-4ad...| 30|Harmony Services ...|Musterstraße 30, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 17|e09ae030-b09a-493...| 17|   Echo Solutions SE|Musterstraße 17, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|ab7038f6-30bd-47a...| 17|   Echo Solutions SE|Musterstraße 17, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "| 50|751e55cc-c5fd-459...| 50|   Mu Enterprises AG|Musterstraße 50, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|14de8bcc-8184-4dc...| 50|   Mu Enterprises AG|Musterstraße 50, ...|          cf|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "|  3|4ece9c60-690c-4cf...|  3|Apex Corporation ...|Musterstraße 3, 1...|    cf uc: 1|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|4c41bdc3-4f1c-4e4...|  3|Apex Corporation ...|Musterstraße 3, 1...|    cf uc: 1|             1|2100-12-31 00:00:00|             0|/edw/hive/psa/dat...|/edw/hive/tmp/dat...|            1|\n",
      "+---+--------------------+---+--------------------+--------------------+------------+--------------+-------------------+--------------+--------------------+--------------------+-------------+--------------------+---+--------------------+--------------------+------------+--------------+-------------------+--------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tmp_client.join(df_client, on=\"pk\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "# update dummy\n",
    "df_tmp_client = df_tmp_client.withColumn(\"name\", f.when(f.col(\"pk\") == 91, \"here_name_changed\").otherwise(f.col(\"name\")))\n",
    "df_tmp_client = df_tmp_client.withColumn(\"name\", f.when(f.col(\"pk\") == 92, \"here_name_changed\").otherwise(f.col(\"name\")))\n",
    "df_tmp_client = df_tmp_client.withColumn(\"name\", f.when(f.col(\"pk\") == 57, \"here_name_changed\").otherwise(f.col(\"name\")))\n",
    "\n",
    "# delete dummy\n",
    "df_tmp_client = df_tmp_client.filter(f.col(\"pk\") != 30)\n",
    "df_tmp_client = df_tmp_client.filter(f.col(\"pk\") != 66)\n",
    "df_tmp_client = df_tmp_client.filter(f.col(\"pk\") != 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/edw/hive/tmp/datev/dbo/client/BATCH_3.parquet \n",
      "\n",
      "/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet /edw/hive/psa/datev/dbo/client/BATCH_0.parquet\n",
      "/edw/hive/tmp/datev/dbo/client/BATCH_1.parquet /edw/hive/psa/datev/dbo/client/BATCH_1.parquet\n",
      "/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet /edw/hive/psa/datev/dbo/client/BATCH_2.parquet\n",
      "/edw/hive/tmp/datev/dbo/client/BATCH_3.parquet /edw/hive/psa/datev/dbo/client/BATCH_3.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# transformations\n",
    "\n",
    "df_client = df_client.withColumn(\"CHANGE_KEY\", f.concat_ws(\"_\", *[\"name\", \"change_field\"]))\n",
    "df_tmp_client_ = df_tmp_client.withColumn(\"CHANGE_KEY_TMP\", f.concat_ws(\"_\", *[\"name\", \"change_field\"]))\n",
    "\n",
    "df_tmp_client_ = df_tmp_client_.withColumnRenamed(\"guid\", \"guid_tmp\")\n",
    "df_tmp_client_ = df_tmp_client_.withColumnRenamed(\"row_filepath\", \"row_new_filepath\")\n",
    "df_tmp_client_ = df_tmp_client_.withColumnRenamed(\"row_filepath_tmp\", \"row_new_tmp_filepath\")\n",
    "\n",
    "df_tmp_client_ = df_tmp_client_.select(*[\"pk\", \"guid_tmp\", \"row_new_filepath\", \"row_new_tmp_filepath\", \"CHANGE_KEY_TMP\"])\n",
    "df_joined = df_client.join(df_tmp_client_, on=\"pk\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_updated_rows = df_joined.filter(f.col(\"CHANGE_KEY\") != f.col(\"CHANGE_KEY_TMP\"))\n",
    "df_deleted_rows = df_joined.filter(f.col(\"CHANGE_KEY_TMP\").isNull())\n",
    "\n",
    "\n",
    "df_updated_rows_pd = pd.DataFrame(\n",
    "    df_updated_rows.collect(),\n",
    "    columns=df_updated_rows.columns\n",
    ")\n",
    "df_deleted_rows_pd = pd.DataFrame(\n",
    "    df_deleted_rows.collect(),\n",
    "    columns=df_deleted_rows.columns\n",
    ")\n",
    "df_deleted_rows_pd.loc[:, \"row_is_deleted\"] = 1\n",
    "\n",
    "df_changed_rows_pd = pd.concat([df_deleted_rows_pd, df_updated_rows_pd])\n",
    "\n",
    "df_updated_rows_pd.loc[:, \"row_is_current\"] = 0\n",
    "df_updated_rows_pd.loc[:, \"row_valid_to\"] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "df_updated_rows_pd.loc[:, \"row_filepath\"] = df_updated_rows_pd[\"row_new_filepath\"]\n",
    "df_updated_rows_pd.loc[:, \"row_filepath_tmp\"] = df_updated_rows_pd[\"row_new_tmp_filepath\"]\n",
    "\n",
    "updated_rows_dist_filepaths = list(df_updated_rows_pd[\"row_new_tmp_filepath\"].unique())\n",
    "\n",
    "\n",
    "path_tmp = \"/edw/hive/tmp/datev/dbo/client\"\n",
    "path_psa = \"/edw/hive/psa/datev/dbo/client\"\n",
    "all_files = hdfs_client.client.list(path_psa)\n",
    "update_files = [file_name for file_name in all_files if \"BATCH\" in file_name]\n",
    "update_files_sorted = sorted(update_files, reverse=True)\n",
    "\n",
    "if len(update_files_sorted) > 0:\n",
    "    last_update_file: str = update_files_sorted[0]\n",
    "    last_number = int(last_update_file.rsplit(\".\", 1)[0].split(\"_\")[1])\n",
    "\n",
    "    last_update_filepath = \"/\".join([path_tmp, last_update_file])\n",
    "\n",
    "    df_last_delete_file = hdfs_client.read_parquet(last_update_filepath)\n",
    "\n",
    "    write_path = f\"{path_tmp}/{last_update_file}\"\n",
    "    if df_last_delete_file.shape[0] > 10_000:\n",
    "        new_update_filename = f\"UPDATE_{last_number + 1}.parquet\"\n",
    "        write_path = f\"{path_tmp}/{new_update_filename}\"\n",
    "\n",
    "    data_content = df_updated_rows_pd.to_parquet(index=False, compression=\"snappy\")\n",
    "    # hdfs_client.write(\n",
    "    #     data=data_content,\n",
    "    #     hdfs_path=write_path,\n",
    "    #     append=True\n",
    "    # )\n",
    "    print(write_path, \"\\n\")\n",
    "else:\n",
    "    write_path = f\"{path_tmp}/UPDATE_0.parquet\"\n",
    "   \n",
    "    data_content = df_updated_rows_pd.to_parquet(index=False, compression=\"snappy\")\n",
    "    print(write_path, \"\\n\")\n",
    "    # hdfs_client.write(\n",
    "    #     data=data_content,\n",
    "    #     hdfs_path=write_path,\n",
    "    #     append=True\n",
    "    # )\n",
    "\n",
    "for update_filename in update_files:\n",
    "    old_filepath = \"/\".join([path_psa, update_filename])\n",
    "    new_filepath = \"/\".join([path_tmp, update_filename])\n",
    "    print(old_filepath, new_filepath)\n",
    "    # hdfs_client.client.rename(\n",
    "    #     hdfs_src_path=old_filepath,\n",
    "    #     hdfs_dst_path=new_filepath\n",
    "    # )\n",
    "\n",
    "    # df_current = hdfs_client.read_parquet(filepath_tmp)\n",
    "    # df_current = df_current[~df_current[\"guid\"].isin(df_updates[\"guid_tmp\"])]\n",
    "\n",
    "    # df_updates = df_updates[base_columns]\n",
    "    # df_transformed = pd.concat([df_updates, df_current], axis=0)\n",
    "\n",
    "    # display(df_updates, df_current, df_transformed)\n",
    "\n",
    "\n",
    "\n",
    "    # print(filepath_tmp)\n",
    "    # display(df_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70466930-83c5-4820-9090-ff2399883d63</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>Xcel Ventures AG</td>\n",
       "      <td>Musterstraße 92, 10198 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15464f6f-402d-4922-b575-28138a6b7751</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>Wavelength Ventures AG</td>\n",
       "      <td>Musterstraße 91, 10197 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                    name  \\\n",
       "0  70466930-83c5-4820-9090-ff2399883d63  92  92        Xcel Ventures AG   \n",
       "1  15464f6f-402d-4922-b575-28138a6b7751  91  91  Wavelength Ventures AG   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 92, 10198 Berlin           cf               0   \n",
       "1  Musterstraße 91, 10197 Berlin           cf               0   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2024-01-04 00:00:00               0   \n",
       "1  2024-01-04 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>Momentum Innovations SE</td>\n",
       "      <td>Musterstraße 49, 10155 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>56fb3c9f-e7f4-4712-980d-12a1a3115265</td>\n",
       "      <td>49</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     name                        address change_field  \\\n",
       "0  49  Momentum Innovations SE  Musterstraße 49, 10155 Berlin           cf   \n",
       "\n",
       "                                   guid  pk         row_valid_to  \\\n",
       "0  56fb3c9f-e7f4-4712-980d-12a1a3115265  49  2100-12-31 00:00:00   \n",
       "\n",
       "   row_is_current  row_is_deleted  \\\n",
       "0               1               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70466930-83c5-4820-9090-ff2399883d63</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>Xcel Ventures AG</td>\n",
       "      <td>Musterstraße 92, 10198 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15464f6f-402d-4922-b575-28138a6b7751</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>Wavelength Ventures AG</td>\n",
       "      <td>Musterstraße 91, 10197 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56fb3c9f-e7f4-4712-980d-12a1a3115265</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Momentum Innovations SE</td>\n",
       "      <td>Musterstraße 49, 10155 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                     name  \\\n",
       "0  70466930-83c5-4820-9090-ff2399883d63  92  92         Xcel Ventures AG   \n",
       "1  15464f6f-402d-4922-b575-28138a6b7751  91  91   Wavelength Ventures AG   \n",
       "0  56fb3c9f-e7f4-4712-980d-12a1a3115265  49  49  Momentum Innovations SE   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 92, 10198 Berlin           cf               0   \n",
       "1  Musterstraße 91, 10197 Berlin           cf               0   \n",
       "0  Musterstraße 49, 10155 Berlin           cf               1   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2024-01-04 00:00:00               0   \n",
       "1  2024-01-04 00:00:00               0   \n",
       "0  2100-12-31 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70466930-83c5-4820-9090-ff2399883d63</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>Xcel Ventures AG</td>\n",
       "      <td>Musterstraße 92, 10198 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15464f6f-402d-4922-b575-28138a6b7751</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>Wavelength Ventures AG</td>\n",
       "      <td>Musterstraße 91, 10197 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_2....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_2.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                    name  \\\n",
       "0  70466930-83c5-4820-9090-ff2399883d63  92  92        Xcel Ventures AG   \n",
       "1  15464f6f-402d-4922-b575-28138a6b7751  91  91  Wavelength Ventures AG   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 92, 10198 Berlin           cf               0   \n",
       "1  Musterstraße 91, 10197 Berlin           cf               0   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2024-01-04 00:00:00               0   \n",
       "1  2024-01-04 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH_2....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH_2.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>805656fe-4ef7-48e5-8fb5-4b50bc45a73d</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>Omicron Systems GmbH</td>\n",
       "      <td>Musterstraße 57, 10163 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                  name  \\\n",
       "2  805656fe-4ef7-48e5-8fb5-4b50bc45a73d  57  57  Omicron Systems GmbH   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "2  Musterstraße 57, 10163 Berlin           cf               0   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "2  2024-01-04 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Epic Solutions SE</td>\n",
       "      <td>Musterstraße 19, 10125 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>e6bbb303-dcf2-409f-bde5-410c5dc1ec23</td>\n",
       "      <td>19</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>Quantum Consulting SE</td>\n",
       "      <td>Musterstraße 66, 10172 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>d73b1652-55e3-40a7-a4f9-34a1b8c53fc0</td>\n",
       "      <td>66</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   name                        address change_field  \\\n",
       "1  19      Epic Solutions SE  Musterstraße 19, 10125 Berlin           cf   \n",
       "2  66  Quantum Consulting SE  Musterstraße 66, 10172 Berlin           cf   \n",
       "\n",
       "                                   guid  pk         row_valid_to  \\\n",
       "1  e6bbb303-dcf2-409f-bde5-410c5dc1ec23  19  2100-12-31 00:00:00   \n",
       "2  d73b1652-55e3-40a7-a4f9-34a1b8c53fc0  66  2100-12-31 00:00:00   \n",
       "\n",
       "   row_is_current  row_is_deleted  \\\n",
       "1               1               0   \n",
       "2               1               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>805656fe-4ef7-48e5-8fb5-4b50bc45a73d</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>Omicron Systems GmbH</td>\n",
       "      <td>Musterstraße 57, 10163 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e6bbb303-dcf2-409f-bde5-410c5dc1ec23</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Epic Solutions SE</td>\n",
       "      <td>Musterstraße 19, 10125 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d73b1652-55e3-40a7-a4f9-34a1b8c53fc0</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>Quantum Consulting SE</td>\n",
       "      <td>Musterstraße 66, 10172 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                   name  \\\n",
       "2  805656fe-4ef7-48e5-8fb5-4b50bc45a73d  57  57   Omicron Systems GmbH   \n",
       "1  e6bbb303-dcf2-409f-bde5-410c5dc1ec23  19  19      Epic Solutions SE   \n",
       "2  d73b1652-55e3-40a7-a4f9-34a1b8c53fc0  66  66  Quantum Consulting SE   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "2  Musterstraße 57, 10163 Berlin           cf               0   \n",
       "1  Musterstraße 19, 10125 Berlin           cf               1   \n",
       "2  Musterstraße 66, 10172 Berlin           cf               1   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "2  2024-01-04 00:00:00               0   \n",
       "1  2100-12-31 00:00:00               0   \n",
       "2  2100-12-31 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>805656fe-4ef7-48e5-8fb5-4b50bc45a73d</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>Omicron Systems GmbH</td>\n",
       "      <td>Musterstraße 57, 10163 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH_0....</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH_0.parquet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  pk  id                  name  \\\n",
       "2  805656fe-4ef7-48e5-8fb5-4b50bc45a73d  57  57  Omicron Systems GmbH   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "2  Musterstraße 57, 10163 Berlin           cf               0   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "2  2024-01-04 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH_0....   \n",
       "\n",
       "                                 row_filepath_tmp  is_batch_file  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH_0.parquet              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HdfsError",
     "evalue": "File /edw/hive/tmp/datev/dbo/client/BATCH2.parquet not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 71\u001b[0m\n\u001b[1;32m     50\u001b[0m     display(df_updates)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#display(df_updated_rows_pd)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# df_deleted_rows_pd = pd.DataFrame(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# df_updated_rows.show()\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# df_deleted_rows.show()\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mhdfs_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/edw/hive/tmp/datev/dbo/client/BATCH2.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# UND --> wenn filename doppelt! nur einmal öffnen und verändern!\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# UPDATE UND DELETE FASSE ICH ABER AUCH NUR EINMAL AN!\u001b[39;00m\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/test/notebooks/../../etl/clients.py:58\u001b[0m, in \u001b[0;36mHDFileSystemClient.read_parquet\u001b[0;34m(self, hdfs_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m df: pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[0;32m---> 58\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbyte_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/hdfs/client.py:725\u001b[0m, in \u001b[0;36mClient.read\u001b[0;34m(self, hdfs_path, offset, length, buffer_size, encoding, chunk_size, delimiter, progress)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelimiter splitting incompatible with chunk size.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    724\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[0;32m--> 725\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m  \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delimiter:\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/hdfs/client.py:118\u001b[0m, in \u001b[0;36m_Request.to_method.<locals>.api_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetriableException\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandbyException\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[0;32m--> 118\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    121\u001b[0m attempted_hosts\u001b[38;5;241m.\u001b[39madd(host)\n",
      "\u001b[0;31mHdfsError\u001b[0m: File /edw/hive/tmp/datev/dbo/client/BATCH2.parquet not found."
     ]
    }
   ],
   "source": [
    "\n",
    "# transformations\n",
    "\n",
    "df_client = df_client.withColumn(\"CHANGE_KEY\", f.concat_ws(\"_\", *[\"name\", \"change_field\"]))\n",
    "df_tmp_client_ = df_tmp_client.withColumn(\"CHANGE_KEY_TMP\", f.concat_ws(\"_\", *[\"name\", \"change_field\"]))\n",
    "\n",
    "df_tmp_client_ = df_tmp_client_.withColumnRenamed(\"guid\", \"guid_tmp\")\n",
    "df_tmp_client_ = df_tmp_client_.withColumnRenamed(\"row_filepath\", \"row_new_filepath\")\n",
    "df_tmp_client_ = df_tmp_client_.withColumnRenamed(\"row_filepath_tmp\", \"row_new_tmp_filepath\")\n",
    "\n",
    "\n",
    "df_tmp_client_ = df_tmp_client_.select(*[\"pk\", \"guid_tmp\", \"row_new_filepath\", \"row_new_tmp_filepath\", \"CHANGE_KEY_TMP\"])\n",
    "df_joined = df_client.join(df_tmp_client_, on=\"pk\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_updated_rows = df_joined.filter(f.col(\"CHANGE_KEY\") != f.col(\"CHANGE_KEY_TMP\"))\n",
    "df_deleted_rows = df_joined.filter(f.col(\"CHANGE_KEY_TMP\").isNull())\n",
    "\n",
    "\n",
    "df_updated_rows_pd = pd.DataFrame(\n",
    "    df_updated_rows.collect(),\n",
    "    columns=df_updated_rows.columns\n",
    ")\n",
    "\n",
    "df_updated_rows_pd.loc[:, \"row_is_current\"] = 0\n",
    "df_updated_rows_pd.loc[:, \"row_valid_to\"] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "df_updated_rows_pd.loc[:, \"row_filepath\"] = df_updated_rows_pd[\"row_new_filepath\"]\n",
    "df_updated_rows_pd.loc[:, \"row_filepath_tmp\"] = df_updated_rows_pd[\"row_new_tmp_filepath\"]\n",
    "\n",
    "updated_rows_dist_filepaths = list(df_updated_rows_pd[\"row_new_tmp_filepath\"].unique())\n",
    "for filepath_tmp in updated_rows_dist_filepaths:\n",
    "    df_updates = df_updated_rows_pd[df_updated_rows_pd[\"row_new_tmp_filepath\"] == filepath_tmp]\n",
    "\n",
    "    df_current = hdfs_client.read_parquet(filepath_tmp)\n",
    "    df_current = df_current[~df_current[\"guid\"].isin(df_updates[\"guid_tmp\"])]\n",
    "\n",
    "    df_updates = df_updates[base_columns]\n",
    "    df_transformed = pd.concat([df_updates, df_current], axis=0)\n",
    "\n",
    "    display(df_updates, df_current, df_transformed)\n",
    "    # data_content = df_transformed.to_parquet(index=False, compression=\"snappy\")\n",
    "    # hdfs_client.write(\n",
    "    #     data=data_content,\n",
    "    #     hdfs_path=filepath\n",
    "    # )\n",
    "\n",
    "\n",
    "    print(filepath_tmp)\n",
    "    display(df_updates)\n",
    "#display(df_updated_rows_pd)\n",
    "\n",
    "# df_deleted_rows_pd = pd.DataFrame(\n",
    "#     df_deleted_rows.collect(),\n",
    "#     columns=df_deleted_rows.columns\n",
    "# )\n",
    "# display(df_deleted_rows_pd) ==> hier mit write arbeiten -> einfach alles reinschreiben! SPARK!\n",
    "    # NEIN => DELETE ALS DELETE FILES SPEICHERN UND FILTERN -> DIESE EINFACH MIT ÜBERNEHMEN\n",
    "\n",
    "# READ FILE\n",
    "# TRANSFORM IT\n",
    "#   - change and add old dataset ==> need GUID & FILEPATH from old dataset\n",
    "#   => result_df = df_test.groupby('path')['guid'].agg(list).to_dict()\n",
    "# WRITE IT BACK (override!)\n",
    "# ==> MIT PANDAS!\n",
    "\n",
    "# print(\"CHANGED COLUMNS:\")\n",
    "# df_updated_rows.show()\n",
    "# df_deleted_rows.show()\n",
    "\n",
    "hdfs_client.read_parquet(\"/edw/hive/tmp/datev/dbo/client/BATCH2.parquet\")\n",
    "print(\"check\")\n",
    "\n",
    "# UND --> wenn filename doppelt! nur einmal öffnen und verändern!\n",
    "\n",
    "# UPDATE UND DELETE FASSE ICH ABER AUCH NUR EINMAL AN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>CHANGE_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>row_new_filepath</th>\n",
       "      <th>row_new_tmp_filepath</th>\n",
       "      <th>CHANGE_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>25fe0eb6-16f2-4d7b-b68e-4d93aa7a65f9</td>\n",
       "      <td>92</td>\n",
       "      <td>Xcel Ventures AG</td>\n",
       "      <td>Musterstraße 92, 10198 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Xcel Ventures AG_cf</td>\n",
       "      <td>aa3e617e-b24e-41c9-9dd7-da339a3311c6</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>here_name_changed_cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>932cd61c-a4a4-48e9-94c9-c43e0d5eca3d</td>\n",
       "      <td>91</td>\n",
       "      <td>Wavelength Ventures AG</td>\n",
       "      <td>Musterstraße 91, 10197 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Wavelength Ventures AG_cf</td>\n",
       "      <td>6906d921-35de-4bc2-989f-27f425c42c34</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>here_name_changed_cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>af764816-3856-4ec0-9c01-36b06f7a8572</td>\n",
       "      <td>57</td>\n",
       "      <td>Omicron Systems GmbH</td>\n",
       "      <td>Musterstraße 57, 10163 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH0.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH0.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Omicron Systems GmbH_cf</td>\n",
       "      <td>b46d271e-bb19-4012-a473-e0603567f512</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH0.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH0.parquet</td>\n",
       "      <td>here_name_changed_cf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pk                                  guid  id                    name  \\\n",
       "0  92  25fe0eb6-16f2-4d7b-b68e-4d93aa7a65f9  92        Xcel Ventures AG   \n",
       "1  91  932cd61c-a4a4-48e9-94c9-c43e0d5eca3d  91  Wavelength Ventures AG   \n",
       "2  57  af764816-3856-4ec0-9c01-36b06f7a8572  57    Omicron Systems GmbH   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 92, 10198 Berlin           cf               1   \n",
       "1  Musterstraße 91, 10197 Berlin           cf               1   \n",
       "2  Musterstraße 57, 10163 Berlin           cf               1   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2100-12-31 00:00:00               0   \n",
       "1  2100-12-31 00:00:00               0   \n",
       "2  2100-12-31 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH0.p...   \n",
       "\n",
       "                                row_filepath_tmp  is_batch_file  \\\n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet              1   \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet              1   \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH0.parquet              1   \n",
       "\n",
       "                  CHANGE_KEY                              guid_tmp  \\\n",
       "0        Xcel Ventures AG_cf  aa3e617e-b24e-41c9-9dd7-da339a3311c6   \n",
       "1  Wavelength Ventures AG_cf  6906d921-35de-4bc2-989f-27f425c42c34   \n",
       "2    Omicron Systems GmbH_cf  b46d271e-bb19-4012-a473-e0603567f512   \n",
       "\n",
       "                                    row_new_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH0.p...   \n",
       "\n",
       "                            row_new_tmp_filepath        CHANGE_KEY_TMP  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet  here_name_changed_cf  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet  here_name_changed_cf  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH0.parquet  here_name_changed_cf  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated_rows_pd = pd.DataFrame(df_updated_rows.collect(), columns=df_updated_rows.columns)\n",
    "df_updated_rows_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk</th>\n",
       "      <th>guid</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>change_field</th>\n",
       "      <th>row_is_current</th>\n",
       "      <th>row_valid_to</th>\n",
       "      <th>row_is_deleted</th>\n",
       "      <th>row_filepath</th>\n",
       "      <th>row_filepath_tmp</th>\n",
       "      <th>is_batch_file</th>\n",
       "      <th>CHANGE_KEY</th>\n",
       "      <th>guid_tmp</th>\n",
       "      <th>row_new_filepath</th>\n",
       "      <th>row_new_tmp_filepath</th>\n",
       "      <th>CHANGE_KEY_TMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>25fe0eb6-16f2-4d7b-b68e-4d93aa7a65f9</td>\n",
       "      <td>92</td>\n",
       "      <td>Xcel Ventures AG</td>\n",
       "      <td>Musterstraße 92, 10198 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Xcel Ventures AG_cf</td>\n",
       "      <td>aa3e617e-b24e-41c9-9dd7-da339a3311c6</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>here_name_changed_cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>932cd61c-a4a4-48e9-94c9-c43e0d5eca3d</td>\n",
       "      <td>91</td>\n",
       "      <td>Wavelength Ventures AG</td>\n",
       "      <td>Musterstraße 91, 10197 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Wavelength Ventures AG_cf</td>\n",
       "      <td>6906d921-35de-4bc2-989f-27f425c42c34</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH2.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH2.parquet</td>\n",
       "      <td>here_name_changed_cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>af764816-3856-4ec0-9c01-36b06f7a8572</td>\n",
       "      <td>57</td>\n",
       "      <td>Omicron Systems GmbH</td>\n",
       "      <td>Musterstraße 57, 10163 Berlin</td>\n",
       "      <td>cf</td>\n",
       "      <td>1</td>\n",
       "      <td>2100-12-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH0.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH0.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>Omicron Systems GmbH_cf</td>\n",
       "      <td>b46d271e-bb19-4012-a473-e0603567f512</td>\n",
       "      <td>/edw/hive/psa/datev/dbo/client/client/BATCH0.p...</td>\n",
       "      <td>/edw/hive/tmp/datev/dbo/client/BATCH0.parquet</td>\n",
       "      <td>here_name_changed_cf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pk                                  guid  id                    name  \\\n",
       "0  92  25fe0eb6-16f2-4d7b-b68e-4d93aa7a65f9  92        Xcel Ventures AG   \n",
       "1  91  932cd61c-a4a4-48e9-94c9-c43e0d5eca3d  91  Wavelength Ventures AG   \n",
       "2  57  af764816-3856-4ec0-9c01-36b06f7a8572  57    Omicron Systems GmbH   \n",
       "\n",
       "                         address change_field  row_is_current  \\\n",
       "0  Musterstraße 92, 10198 Berlin           cf               1   \n",
       "1  Musterstraße 91, 10197 Berlin           cf               1   \n",
       "2  Musterstraße 57, 10163 Berlin           cf               1   \n",
       "\n",
       "          row_valid_to  row_is_deleted  \\\n",
       "0  2100-12-31 00:00:00               0   \n",
       "1  2100-12-31 00:00:00               0   \n",
       "2  2100-12-31 00:00:00               0   \n",
       "\n",
       "                                        row_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH0.p...   \n",
       "\n",
       "                                row_filepath_tmp  is_batch_file  \\\n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet              1   \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet              1   \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH0.parquet              1   \n",
       "\n",
       "                  CHANGE_KEY                              guid_tmp  \\\n",
       "0        Xcel Ventures AG_cf  aa3e617e-b24e-41c9-9dd7-da339a3311c6   \n",
       "1  Wavelength Ventures AG_cf  6906d921-35de-4bc2-989f-27f425c42c34   \n",
       "2    Omicron Systems GmbH_cf  b46d271e-bb19-4012-a473-e0603567f512   \n",
       "\n",
       "                                    row_new_filepath  \\\n",
       "0  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "1  /edw/hive/psa/datev/dbo/client/client/BATCH2.p...   \n",
       "2  /edw/hive/psa/datev/dbo/client/client/BATCH0.p...   \n",
       "\n",
       "                            row_new_tmp_filepath        CHANGE_KEY_TMP  \n",
       "0  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet  here_name_changed_cf  \n",
       "1  /edw/hive/tmp/datev/dbo/client/BATCH2.parquet  here_name_changed_cf  \n",
       "2  /edw/hive/tmp/datev/dbo/client/BATCH0.parquet  here_name_changed_cf  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated_rows_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accessTime': 0,\n",
       " 'blockSize': 0,\n",
       " 'childrenNum': 1,\n",
       " 'fileId': 40344,\n",
       " 'group': 'supergroup',\n",
       " 'length': 0,\n",
       " 'modificationTime': 1704281132017,\n",
       " 'owner': 'enricogoerlitz',\n",
       " 'pathSuffix': '',\n",
       " 'permission': '755',\n",
       " 'replication': 0,\n",
       " 'storagePolicy': 0,\n",
       " 'type': 'DIRECTORY'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.client.status(\"/edw/hive/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "File /edw/hive/tmp/datev/dbo/client/BATCH8.parquet not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m parquet_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/edw/hive/tmp/datev/dbo/client/BATCH8.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mhdfs_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/test/notebooks/../../etl/clients.py:58\u001b[0m, in \u001b[0;36mHDFileSystemClient.read_parquet\u001b[0;34m(self, hdfs_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m df: pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[0;32m---> 58\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbyte_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/hdfs/client.py:725\u001b[0m, in \u001b[0;36mClient.read\u001b[0;34m(self, hdfs_path, offset, length, buffer_size, encoding, chunk_size, delimiter, progress)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelimiter splitting incompatible with chunk size.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    724\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[0;32m--> 725\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m  \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delimiter:\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/hdfs/client.py:118\u001b[0m, in \u001b[0;36m_Request.to_method.<locals>.api_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetriableException\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandbyException\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[0;32m--> 118\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    121\u001b[0m attempted_hosts\u001b[38;5;241m.\u001b[39madd(host)\n",
      "\u001b[0;31mHdfsError\u001b[0m: File /edw/hive/tmp/datev/dbo/client/BATCH8.parquet not found."
     ]
    }
   ],
   "source": [
    "parquet_file_path = '/edw/hive/tmp/datev/dbo/client/BATCH8.parquet'\n",
    "\n",
    "hdfs_client.read_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "File /edw/hive/tmp/datev/dbo/client/BATCH8.parquet not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      4\u001b[0m parquet_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/edw/hive/tmp/datev/dbo/client/BATCH8.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhdfs_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(f.read())\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#print(f.read())\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/hdfs/client.py:725\u001b[0m, in \u001b[0;36mClient.read\u001b[0;34m(self, hdfs_path, offset, length, buffer_size, encoding, chunk_size, delimiter, progress)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelimiter splitting incompatible with chunk size.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    724\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[0;32m--> 725\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m  \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delimiter:\n",
      "File \u001b[0;32m~/LDesktop/Dev-Projects/lib/HadoopDataMeshHub/venv/lib/python3.11/site-packages/hdfs/client.py:118\u001b[0m, in \u001b[0;36m_Request.to_method.<locals>.api_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetriableException\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandbyException\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[0;32m--> 118\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    121\u001b[0m attempted_hosts\u001b[38;5;241m.\u001b[39madd(host)\n",
      "\u001b[0;31mHdfsError\u001b[0m: File /edw/hive/tmp/datev/dbo/client/BATCH8.parquet not found."
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "\n",
    "parquet_file_path = '/edw/hive/tmp/datev/dbo/client/BATCH8.parquet'\n",
    "with hdfs_client.client.read(parquet_file_path) as f:\n",
    "    display(pd.read_parquet(BytesIO(f.read())))\n",
    "    # print(f.read())\n",
    "    #print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/file/path/1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/file/path/1</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/file/path/1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/file/path/2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/file/path/2</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/file/path/3</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           path  guid\n",
       "0  /file/path/1    20\n",
       "1  /file/path/1    89\n",
       "2  /file/path/1    10\n",
       "3  /file/path/2    11\n",
       "4  /file/path/2    90\n",
       "5  /file/path/3   101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    \"path\": [\"/file/path/1\", \"/file/path/1\", \"/file/path/1\", \"/file/path/2\", \"/file/path/2\", \"/file/path/3\"],\n",
    "    \"guid\": [20, 89, 10, 11, 90, 101]\n",
    "})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/file/path/1': [20, 89, 10], '/file/path/2': [11, 90], '/file/path/3': [101]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = df_test.groupby('path')['guid'].agg(list).to_dict()\n",
    "\n",
    "# Umbenennen des Index in 'path'\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
